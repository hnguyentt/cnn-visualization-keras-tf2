# CNN Visualization and Explanation
This work aims to:
* Visualize filters and feature maps of all pre-trained models on ImageNet in [`tf.keras.applications`](https://github.com/conan7882/CNN-Visualization) with `Tensorflow` verion 2.3.0. The visualization methods include simply plotting filters of the model, plotting the feature maps of convolutional layers, DeConvNet and Guided Backpropagation
* Explain for the top 5 predictions of these models by GradCAM and Guided-GradCAM
* Generate Deep Dream visuals.

With the current version, there are 26 pre-trained models.

## How to use
### Run with your resource
* Clone this repo:
```bash
git clone https://github.com/nguyenhoa93/cnn-visualization-keras-tf2
cd cnn-visualization-keras-tf2
```
* Create virtualev:
```bash
conda create -n cnn-vis python=3.6
conda activate cnn-vs
bash requirements.txt
```
* Run demo with the file `visualization.ipynb`

### Run on Google Colab
* Go to this link: https://colab.research.google.com/github/nguyenhoa93/cnn-visualization-keras-tf2/blob/master/visualization.ipynb
* Change your runtime type to `Python3`
* Choose GPU as your hardware accelerator.
* Run the code.

Voila! You got it.

![](images/demo-1.gif)

## Briefs

<table style="border-collapse:collapse;border-spacing:0;table-layout: fixed; width: 717px" class="tg"><colgroup><col style="width: 191px"><col style="width: 526px"></colgroup><thead><tr><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal"><span style="font-weight:bold">Method</span></th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal"><span style="font-weight:bold">Brief</span></th></tr></thead><tbody><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Filter visualization</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Simply plot the learned filters.<br>* Step 1: Find a convolutional layer.<br>* Step 2: Get weights at a convolution layer, they are filters at this layer.<br>* Step 3: Plot filter with the values from step 2.<br>This method does not require an input image.<br><br><span style="font-weight:bold;font-style:italic">VGG-16, block1_conv1</span><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/filtervisVGG16_block1_conv1.png" alt="Image" width="500" height="178"></td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Feature map visualization</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Plot the feature maps obtained when fitting an image to the network.<br>* Step 1: Find a convolutional layer.<br>* Step 2: Build a feature model from the input up to that convolutional layer.<br>* Step 3: Fit the image to the feature model to get feature maps.<br>* Step 4: Plot the feature map.<br><br><span style="font-weight:bold;font-style:italic">VGG-16, block1_conv1</span><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/featurevisVGG16_block1_conv1.png" alt="Image" width="500" height="394"><br><br><span style="font-weight:bold">VGG-16, block5_conv3</span><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/featurevisVGG16_block5_conv3.png" alt="Image" width="500" height="394"></td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Guided Backpropagation</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Backpropagate from a particular convolution layer to input image with modificaton of the gradient of ReLU.<br><br><span style="font-weight:bold">VGG-16: block1_conv1 &amp; block5_conv3</span><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/guidedbackpropVGG16_block1_conv1.png" alt="Image" width="231" height="231"><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/backguidedVGG16_block5_conv3.png" alt="Image" width="231" height="231"></td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">GradCAM</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">* Step 1: Determine the last convolutional layer<br>* Step 2: Perform gradient from `pre-softmax` layer to last convolutional layer and the apply global average pooling to obtain weights for neurons' importance.<br>* Step 3: Linearly combinate feature map of last convolutional layer and weights, then apply ReLu on that linear combination.<br><br><span style="font-weight:bold;font-style:italic">InceptionV3, explanation for "lakeside" class</span><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/lakesideInceptionV3.png" alt="Image" width="460" height="233"></td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Guided-GradCAM</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">* Step 1: Calculate guided backpropagation from last convolutional layer to input.<br>* Step 2: Upsample GradCAM to the size of input<br>* Step 3: Apply element-wise multiplication of guided backpropagation and GradCAM<br><br><span style="font-weight:700;font-style:italic">InceptionV3, explanation for "lakeside" class</span><br><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/boathouseInceptionv3.png" width="460" height="233"></td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Deep Dream</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">See more in this excellent tutorial from François Chollet: <a href="https://keras.io/examples/generative/deep_dream/" target="_blank" rel="noopener noreferrer">https://keras.io/examples/generative/deep_dream/</a><br><br><span style="font-weight:bold;font-style:italic">InceptionV3</span><br><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/deepdreamInceptionv3.png" alt="Image" width="500" height="333"></td></tr><tr><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" colspan="2">Original image<br><br><img src="https://raw.githubusercontent.com/nguyenhoa93/cnn-visualization-keras-tf2/master/images/lapan.jpg" alt="Image" width="720" height="479"></td></tr></tbody></table>

## References
1. [How to Visualize Filters and Feature Maps in Convolutional Neural Networks](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/) by Machine Learning Mastery
2. Pytorch CNN visualzaton by [utkuozbulak](https://github.com/utkuozbulak): https://github.com/utkuozbulak
3. CNN visualization with TF 1.3 by [conan7882](https://github.com/conan7882): https://github.com/conan7882/CNN-Visualization
4. Deep Dream Tutorial from François Chollet: https://keras.io/examples/generative/deep_dream/ 